{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run\n",
    "\n",
    "def show_json(obj):\n",
    "    display(json.loads(obj.model_dump_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration OpenAI Beta Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"YOUR API KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math tutor\",\n",
    "    instructions=\"You are a personal Math tutor. Answer questions briefly, in a sentence or less.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "#show_json(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'thread_5KOcCvRjMUuu4WhVI2XVQbLO',\n",
       " 'created_at': 1727716264,\n",
       " 'metadata': {},\n",
       " 'object': 'thread',\n",
       " 'tool_resources': {'code_interpreter': None, 'file_search': None}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thread = client.beta.threads.create()\n",
    "show_json(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In mathematics, the sum of 1 and 1 is 2 because when you combine one item with another item, you have a total of two items.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Explain me why 1 + 1 = 2?\",\n",
    ")\n",
    "#show_json(message)\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "#show_json(run)\n",
    "\n",
    "run = wait_on_run(run, thread)\n",
    "#show_json(run)\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id))\n",
    "#show_json(messages)\n",
    "\n",
    "print(messages[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En mathématiques, la somme de 1 et 1 est égale à 2 car lorsque vous combinez un élément avec un autre élément, vous obtenez un total de deux éléments.\n"
     ]
    }
   ],
   "source": [
    "# Create a message to append to our thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id, role=\"user\", content=\"Can you translate the last response to me in french?\"\n",
    ")\n",
    "\n",
    "# Execute our run\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "wait_on_run(run, thread)\n",
    "\n",
    "# Retrieve all the messages added after our last user message\n",
    "messages = list(client.beta.threads.messages.list(\n",
    "    thread_id=thread.id, order=\"asc\", after=message.id\n",
    "))\n",
    "#show_json(messages)\n",
    "\n",
    "print(messages[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My last two answers were explaining why 1 + 1 = 2 in mathematics, and then translating that explanation into French.\n"
     ]
    }
   ],
   "source": [
    "# Create a message to append to our thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id, role=\"user\", content=\"tell me what were the last two answers from you?\"\n",
    ")\n",
    "\n",
    "# Execute our run\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "wait_on_run(run, thread)\n",
    "\n",
    "# Retrieve all the messages added after our last user message\n",
    "messages = list(client.beta.threads.messages.list(\n",
    "    thread_id=thread.id, order=\"asc\", after=message.id\n",
    "))\n",
    "#show_json(messages)\n",
    "\n",
    "print(messages[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration OpenAI Beta Vectorstore together with the OpenAI Beta Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_ewV4u5rfzsMR6B9C0AvBOZuu',\n",
       " 'created_at': 1727716375,\n",
       " 'description': None,\n",
       " 'instructions': 'You are a personal File search expert. Answer questions briefly, in a sentence or less.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'name': 'File Search Expert',\n",
       " 'object': 'assistant',\n",
       " 'tools': [{'type': 'file_search',\n",
       "   'file_search': {'max_num_results': None,\n",
       "    'ranking_options': {'ranker': 'default_2024_08_21',\n",
       "     'score_threshold': 0.0}}}],\n",
       " 'response_format': 'auto',\n",
       " 'temperature': 1.0,\n",
       " 'tool_resources': {'code_interpreter': None,\n",
       "  'file_search': {'vector_store_ids': []}},\n",
       " 'top_p': 1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"File Search Expert\",\n",
    "    instructions=\"You are a personal File search expert. Answer questions briefly, in a sentence or less.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the list of files from the user\n",
    "file_paths = ['files/Article-1.pdf','files/CV_1.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a vector store and upload files\n",
    "vector_store = client.beta.vector_stores.create(name=\"User Files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the files for upload\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Upload files and add them to the vector store\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=2, failed=0, in_progress=0, total=2)\n"
     ]
    }
   ],
   "source": [
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Update the assistant to use the new Vector Store\n",
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article-1 discusses the comparison between open-source Large Language Models (LLMs) and proprietary models like ChatGPT, focusing on factors like data protection, governance, and adoption considerations【4:0†source】.\n"
     ]
    }
   ],
   "source": [
    "# Create a message to append to our thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id, role=\"user\", content=\"What is article-1 about?\"\n",
    ")\n",
    "\n",
    "# Execute our run\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "wait_on_run(run, thread)\n",
    "\n",
    "# Retrieve all the messages added after our last user message\n",
    "messages = list(client.beta.threads.messages.list(\n",
    "    thread_id=thread.id, order=\"asc\", after=message.id\n",
    "))\n",
    "#show_json(messages)\n",
    "\n",
    "print(messages[0].content[0].text.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'article 1 discute de la comparaison entre les grands modèles de langage à source ouverte (LLMs) et les modèles propriétaires comme ChatGPT, en se concentrant sur des facteurs tels que la protection des données, la gouvernance et les considérations d'adoption【4:0†source】.\n"
     ]
    }
   ],
   "source": [
    "# Create a message to append to our thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id, role=\"user\", content=\"Translate the last response to french\"\n",
    ")\n",
    "\n",
    "# Execute our run\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "wait_on_run(run, thread)\n",
    "\n",
    "# Retrieve all the messages added after our last user message\n",
    "messages = list(client.beta.threads.messages.list(\n",
    "    thread_id=thread.id, order=\"asc\", after=message.id\n",
    "))\n",
    "#show_json(messages)\n",
    "\n",
    "print(messages[0].content[0].text.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
